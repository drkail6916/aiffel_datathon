{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0391d03c",
   "metadata": {
    "id": "VWoBF-eN5NCr",
    "papermill": {
     "duration": 0.013837,
     "end_time": "2025-09-19T05:21:19.463093",
     "exception": false,
     "start_time": "2025-09-19T05:21:19.449256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4369111",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-19T05:21:19.490444Z",
     "iopub.status.busy": "2025-09-19T05:21:19.490022Z",
     "iopub.status.idle": "2025-09-19T05:21:19.613144Z",
     "shell.execute_reply": "2025-09-19T05:21:19.612006Z"
    },
    "id": "7f6bb187",
    "outputId": "b1e23623-820d-4889-c2d4-c48296528470",
    "papermill": {
     "duration": 0.139156,
     "end_time": "2025-09-19T05:21:19.614936",
     "exception": false,
     "start_time": "2025-09-19T05:21:19.475780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 2: syntax error: unexpected end of file\r\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d903911",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-19T05:21:19.643606Z",
     "iopub.status.busy": "2025-09-19T05:21:19.642700Z",
     "iopub.status.idle": "2025-09-19T05:21:23.422836Z",
     "shell.execute_reply": "2025-09-19T05:21:23.421613Z"
    },
    "id": "hGuU8fmUx-2e",
    "outputId": "baaeaabd-c7da-4538-c8c2-3da5d881e2fb",
    "papermill": {
     "duration": 3.794911,
     "end_time": "2025-09-19T05:21:23.424099",
     "exception": true,
     "start_time": "2025-09-19T05:21:19.629188",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/DS/accepted_2015.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/3202113237.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/DS/accepted_2015.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/DS/accepted_2015.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/DS/accepted_2015.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8bb6a9",
   "metadata": {
    "id": "6b0Nm8nY_8gc",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf99abd",
   "metadata": {
    "id": "gzLz1Qch_-RM",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# emp_title --> 직업별 평균 연소득(emp_title_avg_inc)\n",
    "\n",
    "import re\n",
    "\n",
    "def func_emp_title(x):\n",
    "  cleaned_text = re.sub(r'\\t', '', str(x).lower()) # 탭(\\t) 제거\n",
    "\n",
    "  cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text) # [^\\w\\s]는 알파벳, 숫자, 밑줄, 공백을 제외한 문자를 공백(' ')으로 변환\n",
    "\n",
    "  cleaned_text = re.sub(r'\\s+', ' ', cleaned_text) # 여러 개의 공백을 공백 하나로 통합\n",
    "  return cleaned_text\n",
    "\n",
    "df['emp_title'] = df['emp_title'].apply(func_emp_title)\n",
    "\n",
    "# 직업별 평균 년소득 특징 생성 (emp_title_avg_annual_inc 활용)\n",
    "emp_title_avg_annual_inc = df.groupby('emp_title')['annual_inc'].mean()\n",
    "\n",
    "df['emp_title_avg_inc'] = df['emp_title'].map(emp_title_avg_annual_inc)\n",
    "df = df.drop('emp_title', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3964eee",
   "metadata": {
    "id": "TcxIy8bJAMEg",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# addr_state --> addr_state_avg_annual_inc\n",
    "\n",
    "addr_state_avg_annual_inc = df.groupby('addr_state')['annual_inc'].mean()\n",
    "\n",
    "df['addr_state_avg_annual_inc'] = df['addr_state'].map(addr_state_avg_annual_inc)\n",
    "df = df.drop(['addr_state'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7fd20b",
   "metadata": {
    "id": "7Ip1I0d0Aq23",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# earliest_cr_line --> 신용거래기간 (credit_history_length)\n",
    "\n",
    "def create_credit_history(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['earliest_cr_line_dt'] = pd.to_datetime(df_copy['earliest_cr_line'], format='%b-%Y')\n",
    "    df_copy['issue_d_dt'] = pd.to_datetime(df_copy['issue_d'], format='%b-%Y')\n",
    "\n",
    "    # 신용거래기간\n",
    "    df_copy['credit_history_length'] = (\n",
    "        (df_copy['issue_d_dt'].dt.year - df_copy['earliest_cr_line_dt'].dt.year) * 12 +\n",
    "        (df_copy['issue_d_dt'].dt.month - df_copy['earliest_cr_line_dt'].dt.month))\n",
    "\n",
    "    return df_copy['credit_history_length']\n",
    "\n",
    "df['credit_history_length'] = create_credit_history(df)\n",
    "df = df.drop(['earliest_cr_line'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d060a4",
   "metadata": {
    "id": "iWirjISsAyQz",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# last_credit_pull_d --> 마지막 신용 조회 후 경과기간(time_since_last_credit_check)\n",
    "\n",
    "def create_time_since_last_credit_check(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['issue_d_dt'] = pd.to_datetime(df_copy['issue_d'], format='%b-%Y')\n",
    "    df_copy['last_credit_pull_d_dt'] = pd.to_datetime(df_copy['last_credit_pull_d'], format='%b-%Y')\n",
    "\n",
    "    # 마지막 신용 조회 기간 계산\n",
    "    df_copy['time_since_last_credit_check'] = (\n",
    "        (df_copy['issue_d_dt'].dt.year - df_copy['last_credit_pull_d_dt'].dt.year) * 12 +\n",
    "        (df_copy['issue_d_dt'].dt.month - df_copy['last_credit_pull_d_dt'].dt.month))\n",
    "\n",
    "    df_copy['time_since_last_credit_check'] = np.where(\n",
    "        df_copy['time_since_last_credit_check'] < 0, 0, df_copy['time_since_last_credit_check'])\n",
    "\n",
    "    return df_copy['time_since_last_credit_check']\n",
    "\n",
    "df['time_since_last_credit_check'] = create_time_since_last_credit_check(df)\n",
    "df = df.drop(['last_credit_pull_d'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6f62b",
   "metadata": {
    "id": "clTnGzybA4rJ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sub_grade --> sub_grade_score\n",
    "\n",
    "grades = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "sub_grades_ordered = [f'{g}{i}' for g in grades for i in range(1, 6)]\n",
    "\n",
    "sub_grade_map = {sub: len(sub_grades_ordered) - i for i, sub in enumerate(sub_grades_ordered)}\n",
    "\n",
    "df['sub_grade_score'] = df['sub_grade'].map(sub_grade_map)\n",
    "df = df.drop(['sub_grade'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168909f",
   "metadata": {
    "id": "tHq5Nv8oBRUc",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# title의 count=1인 case --> other\n",
    "def func_title(df):\n",
    "  tmp = df['title'].value_counts()\n",
    "  others = tmp[tmp<=1].index\n",
    "\n",
    "  df['title'] = df['title'].replace(others, 'Other')\n",
    "  return df\n",
    "\n",
    "df = func_title(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ebe28",
   "metadata": {
    "id": "S6LzRf__1bWS",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['emp_length'] = df['emp_length'].fillna(0)\n",
    "df['title'] = df['title'].fillna('None')\n",
    "df['is_default'] = df['is_default'].map({'Default':True, 'NonDefault':False})\n",
    "df = df.drop(['issue_d', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f46eae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SwJpEbXZ1me_",
    "outputId": "ae76677d-491e-456b-bceb-2781ec54aa7d",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944c300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T01:10:53.313102Z",
     "iopub.status.busy": "2025-09-19T01:10:53.312763Z",
     "iopub.status.idle": "2025-09-19T01:10:58.207161Z",
     "shell.execute_reply": "2025-09-19T01:10:58.206096Z",
     "shell.execute_reply.started": "2025-09-19T01:10:53.313067Z"
    },
    "id": "Ja_lrMl6qKRz",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop('is_default', axis=1)\n",
    "y = df['is_default']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Use stratify for imbalanced target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fef91b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-19T01:10:58.208752Z",
     "iopub.status.busy": "2025-09-19T01:10:58.208295Z",
     "iopub.status.idle": "2025-09-19T01:10:58.215264Z",
     "shell.execute_reply": "2025-09-19T01:10:58.214028Z",
     "shell.execute_reply.started": "2025-09-19T01:10:58.208727Z"
    },
    "id": "Id0M4Nnvth4I",
    "outputId": "5f198d90-8411-4c4b-b7c2-7cc9f40d2fda",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3ef1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "execution": {
     "iopub.execute_input": "2025-09-19T01:10:58.217088Z",
     "iopub.status.busy": "2025-09-19T01:10:58.216669Z",
     "iopub.status.idle": "2025-09-19T01:10:58.360989Z",
     "shell.execute_reply": "2025-09-19T01:10:58.360170Z",
     "shell.execute_reply.started": "2025-09-19T01:10:58.217052Z"
    },
    "id": "GeFdRA5XtlOK",
    "outputId": "4815033c-0399-404c-a854-133eb044f60f",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 클래스 비율\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b2ab0c",
   "metadata": {
    "id": "y-ZbqHMvP4pT",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 모델 학습 (데이터 누수 문제 해결)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cafb89e",
   "metadata": {
    "id": "32xwxk8KRKW-",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## holdout 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8d58c0",
   "metadata": {
    "id": "_QEgOPD-LBlJ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### custom 함수\n",
    "- train_and_evaluate_model(model, X, y)\n",
    "- train_and_evaluate_model_with_log_smote(model, X, y)\n",
    "- plot_feature_importance(model_pipeline)\n",
    "- train_evaluate_and_get_metrics_with_smote(model, X, y)\n",
    "- train_evaluate_and_get_metrics_without_smote(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807d914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T01:10:58.362441Z",
     "iopub.status.busy": "2025-09-19T01:10:58.362102Z",
     "iopub.status.idle": "2025-09-19T01:10:58.511937Z",
     "shell.execute_reply": "2025-09-19T01:10:58.510885Z",
     "shell.execute_reply.started": "2025-09-19T01:10:58.362410Z"
    },
    "id": "ug0pX7Tiib-f",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score # Import roc_auc_score\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline # Use a different name for imblearn Pipeline to avoid conflict\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def train_and_evaluate_model(model, X, y):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a machine learning model.\n",
    "\n",
    "    Args:\n",
    "        model: The machine learning model pipeline to train.\n",
    "        X: The feature DataFrame.\n",
    "        y: The target Series.\n",
    "\n",
    "    Returns:\n",
    "        The trained model pipeline.\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Use stratify\n",
    "\n",
    "    categorical_features = X_train.select_dtypes('object').columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                     ('classifier', model)]) # Use the input model here\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nConfusion Matrix :\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return model_pipeline\n",
    "\n",
    "# plot_feature_importance\n",
    "def plot_feature_importance(model_pipeline):\n",
    "  feature_importances = model_pipeline.named_steps['classifier'].feature_importances_\n",
    "\n",
    "  # Access feature names from the fitted preprocessor\n",
    "  preprocessor = model_pipeline.named_steps['preprocessor']\n",
    "  feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "  feature_importance_series = pd.Series(feature_importances, index=feature_names)\n",
    "\n",
    "  # Sort feature importances and select top N features (e.g., top 20)\n",
    "  top_n = 20\n",
    "  top_features = feature_importance_series.sort_values(ascending=False).head(top_n)\n",
    "\n",
    "  # Plot the top feature importances\n",
    "  plt.figure(figsize=(12, 8))\n",
    "  sns.barplot(x=top_features.values, y=top_features.index)\n",
    "  plt.title('Top {} Feature Importances'.format(top_n))\n",
    "  plt.xlabel('Importance')\n",
    "  plt.ylabel('Feature')\n",
    "\n",
    "  return feature_importance_series.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "def train_and_evaluate_model_with_log_smote(model, X, y):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a machine learning model with log transformation and SMOTE\n",
    "    using imblearn Pipeline.\n",
    "\n",
    "    Args:\n",
    "        model: The machine learning model to train.\n",
    "        X: The feature DataFrame.\n",
    "        y: The target Series.\n",
    "\n",
    "    Returns:\n",
    "        The trained model pipeline.\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    categorical_features = X_train.select_dtypes('object').columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    features_to_log_transform = [\n",
    "        'int_rate', 'installment', 'annual_inc', 'dti', 'open_acc',\n",
    "        'revol_bal', 'total_acc', 'total_rec_int', 'mths_since_rcnt_il', 'il_util',\n",
    "        'max_bal_bc', 'total_rev_hi_lim', 'avg_cur_bal', 'bc_open_to_buy', 'dti_joint'\n",
    "    ]\n",
    "\n",
    "    # Separate numerical features that need log transformation from those that only need scaling\n",
    "    numerical_log_features = [f for f in numerical_features if f in features_to_log_transform]\n",
    "    numerical_scale_only_features = [f for f in numerical_features if f not in features_to_log_transform]\n",
    "\n",
    "    log_transformer = FunctionTransformer(func=np.log1p, validate=False)\n",
    "    numerical_scaler = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Create a column transformer to apply different transformations to different columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num_log', Pipeline([('log', log_transformer), ('scaler', numerical_scaler)]), numerical_log_features),\n",
    "            ('num_scale', numerical_scaler, numerical_scale_only_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Create an imblearn pipeline: preprocessor -> smote -> classifier\n",
    "    smote = SMOTE(random_state=42)\n",
    "    model_pipeline = ImbPipeline(steps=[('preprocessor', preprocessor), # Use ImbPipeline\n",
    "                                     ('smote', smote), # SMOTE 단계 추가\n",
    "                                     ('classifier', model)]) # 모델 단계\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nConfusion Matrix :\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return model_pipeline\n",
    "\n",
    "\n",
    "def train_evaluate_and_get_metrics_with_smote(model, X, y):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a machine learning model with log transformation and SMOTE\n",
    "    using imblearn Pipeline, and returns the trained pipeline and metrics.\n",
    "\n",
    "    Args:\n",
    "        model: The machine learning model to train.\n",
    "        X: The feature DataFrame.\n",
    "        y: The target Series.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The trained model pipeline (imblearn.pipeline.Pipeline).\n",
    "        - A dictionary of evaluation metrics (Accuracy, Precision, Recall, F1-Score for the positive class).\n",
    "        Returns None, None if an error occurs during training or metric calculation.\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # 클래스 비율 유지하며 분할\n",
    "\n",
    "    categorical_features = X_train.select_dtypes('object').columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    features_to_log_transform = [\n",
    "        'int_rate', 'installment', 'annual_inc', 'dti', 'open_acc',\n",
    "        'revol_bal', 'total_acc', 'total_rec_int', 'mths_since_rcnt_il', 'il_util',\n",
    "        'max_bal_bc', 'total_rev_hi_lim', 'avg_cur_bal', 'bc_open_to_buy', 'dti_joint'\n",
    "    ]\n",
    "\n",
    "    # Separate numerical features that need log transformation from those that only need scaling\n",
    "    numerical_log_features = [f for f in numerical_features if f in features_to_log_transform]\n",
    "    numerical_scale_only_features = [f for f in numerical_features if f not in features_to_log_transform]\n",
    "\n",
    "    # Create preprocessing steps\n",
    "    log_transformer = FunctionTransformer(func=np.log1p, validate=False)\n",
    "    numerical_scaler = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Create a column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num_log', Pipeline([('log', log_transformer), ('scaler', numerical_scaler)]), numerical_log_features),\n",
    "            ('num_scale', numerical_scaler, numerical_scale_only_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Create SMOTE instance\n",
    "    smote = SMOTE(random_state=42)\n",
    "\n",
    "    # Create imblearn pipeline\n",
    "    model_pipeline = ImbPipeline(steps=[('preprocessor', preprocessor), # Use ImbPipeline\n",
    "                                     ('smote', smote),\n",
    "                                     ('classifier', model)])\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    try:\n",
    "        model_pipeline.fit(X_train, y_train)\n",
    "        y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        print(\"\\nConfusion Matrix :\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        # Calculate individual metrics for the positive class ('Default' or True)\n",
    "        metrics = {}\n",
    "        # Ensure True is in y_test before calculating metrics for pos_label=True\n",
    "        if True in y_test.unique():\n",
    "             metrics['Accuracy'] = accuracy_score(y_test, y_pred)\n",
    "             metrics['Precision'] = precision_score(y_test, y_pred, pos_label=True)\n",
    "             metrics['Recall'] = recall_score(y_test, y_pred, pos_label=True)\n",
    "             metrics['F1-Score'] = f1_score(y_test, y_pred, pos_label=True)\n",
    "\n",
    "             # Calculate AUC if the model has predict_proba\n",
    "             if hasattr(model_pipeline, 'predict_proba'):\n",
    "                 y_prob = model_pipeline.predict_proba(X_test)[:, 1]\n",
    "                 metrics['AUC'] = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        else:\n",
    "             print(\"Warning: Positive class (True) not found in y_test. Metrics for positive class not calculated.\")\n",
    "\n",
    "\n",
    "        return model_pipeline, metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model training or metric calculation: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def train_evaluate_and_get_metrics_without_smote(model, X, y):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a machine learning model with log transformation\n",
    "    using imblearn Pipeline, without SMOTE.\n",
    "\n",
    "    Args:\n",
    "        model: The machine learning model to train.\n",
    "        X: The feature DataFrame.\n",
    "        y: The target Series.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The trained model pipeline (imblearn.pipeline.Pipeline).\n",
    "        - A dictionary of evaluation metrics (Accuracy, Precision, Recall, F1-Score for the positive class).\n",
    "        Returns None, None if an error occurs during training or metric calculation.\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # 클래스 비율 유지하며 분할\n",
    "\n",
    "    categorical_features = X_train.select_dtypes('object').columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "    features_to_log_transform = [\n",
    "        'int_rate', 'installment', 'annual_inc', 'dti', 'open_acc',\n",
    "        'revol_bal', 'total_acc', 'total_rec_int', 'mths_since_rcnt_il', 'il_util',\n",
    "        'max_bal_bc', 'total_rev_hi_lim', 'avg_cur_bal', 'bc_open_to_buy'\n",
    "    ]\n",
    "    numerical_log_features = [f for f in numerical_features if f in features_to_log_transform]\n",
    "    numerical_scale_only_features = [f for f in numerical_features if f not in features_to_log_transform]\n",
    "\n",
    "    # Create preprocessing steps\n",
    "    log_transformer = FunctionTransformer(func=np.log1p, validate=False)\n",
    "    numerical_scaler = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num_log', Pipeline([('log', log_transformer), ('scaler', numerical_scaler)]), numerical_log_features),\n",
    "            ('num_scale', numerical_scaler, numerical_scale_only_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor), # Use standard scikit-learn Pipeline\n",
    "                                     ('classifier', model)]) # 모델 단계\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    try:\n",
    "        model_pipeline.fit(X_train, y_train)\n",
    "        y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        print(\"\\nConfusion Matrix :\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        metrics = {}\n",
    "        if True in y_test.unique():\n",
    "             metrics['Accuracy'] = accuracy_score(y_test, y_pred)\n",
    "             metrics['Precision'] = precision_score(y_test, y_pred, pos_label=True)\n",
    "             metrics['Recall'] = recall_score(y_test, y_pred, pos_label=True)\n",
    "             metrics['F1-Score'] = f1_score(y_test, y_pred, pos_label=True)\n",
    "\n",
    "             # Calculate AUC if the model has predict_proba\n",
    "             if hasattr(model_pipeline, 'predict_proba'):\n",
    "                 y_prob = model_pipeline.predict_proba(X_test)[:, 1]\n",
    "                 metrics['AUC'] = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "        else:\n",
    "             print(\"Warning: Positive class (True) not found in y_test. Metrics for positive class not calculated.\")\n",
    "\n",
    "\n",
    "        return model_pipeline, metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during model training or metric calculation: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90604ea",
   "metadata": {
    "id": "rU1wGSXZ-TMW",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 학습\n",
    "___\n",
    "첫번째 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a29e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T18:14:37.189693Z",
     "iopub.status.busy": "2025-09-18T18:14:37.189194Z",
     "iopub.status.idle": "2025-09-18T18:14:37.195521Z",
     "shell.execute_reply": "2025-09-18T18:14:37.194333Z",
     "shell.execute_reply.started": "2025-09-18T18:14:37.189667Z"
    },
    "id": "cf6f6448",
    "outputId": "063345b9-1357-42a9-baae-5e5b3fa8be10",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model training\n",
    "X = df.drop('is_default', axis=1)\n",
    "y = df['is_default']\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "model_pipeline = train_and_evaluate_model(model, X, y)\n",
    "\n",
    "\n",
    "# --- 결과 기록 ---\n",
    "# Training the model...\n",
    "# Training complete.\n",
    "\n",
    "# Classification Report:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        False       0.96      1.00      0.98     68609\n",
    "#         True       1.00      0.82      0.90     15488\n",
    "\n",
    "#     accuracy                           0.97     84097\n",
    "#    macro avg       0.98      0.91      0.94     84097\n",
    "# weighted avg       0.97      0.97      0.97     84097\n",
    "\n",
    "\n",
    "# Confusion Matrix :\n",
    "# [[68609     0]\n",
    "#  [ 2787 12701]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f813d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T18:14:37.196882Z",
     "iopub.status.busy": "2025-09-18T18:14:37.196606Z",
     "iopub.status.idle": "2025-09-18T18:14:37.221603Z",
     "shell.execute_reply": "2025-09-18T18:14:37.220407Z",
     "shell.execute_reply.started": "2025-09-18T18:14:37.196860Z"
    },
    "id": "1QMcsb7KRUpx",
    "outputId": "d947eeab-c187-482c-cc30-e8fc5d627e99",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_feature_importance\n",
    "feature_importance = plot_feature_importance(model_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec1d22",
   "metadata": {
    "id": "D07QRCq6JsK2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "두번쨰 학습\n",
    "- features_leakage_related + features_to_check 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df90ffb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBFyC6q0JyAk",
    "outputId": "671ddc32-302c-453d-e0fd-1c3bf1d5950d",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "features_leakage_related = [\n",
    "    'last_fico_range_high', 'last_fico_range_low', 'total_pymnt_inv', 'last_pymnt_amnt', 'out_prncp_inv', 'out_prncp',\n",
    "    'total_rec_prncp', 'total_pymnt', 'recoveries', 'collection_recovery_fee', ]\n",
    "\n",
    "features_highly_correlated = ['fico_range_low', 'loan_amnt', 'funded_amnt_inv', 'funded_amnt', 'num_sats',\n",
    "                          'num_rev_tl_bal_gt_0', 'tot_cur_bal',  'mo_sin_old_rev_tl_op']\n",
    "\n",
    "features_to_check = [\n",
    "    'delinq_2yrs', 'fico_range_high', 'inq_last_6mths', 'pub_rec',\n",
    "    'total_rec_late_fee', 'collections_12_mths_ex_med', 'acc_now_delinq', 'chargeoff_within_12_mths', 'delinq_amnt'\n",
    "]\n",
    "\n",
    "X = df.drop(['is_default'] + features_leakage_related + features_to_check, axis=1)\n",
    "y = df['is_default']\n",
    "\n",
    "# Calculate scale_pos_weight for XGBoost and LightGBM (needed for Case 2 models)\n",
    "# scale_pos_weight = count(negative class) / count(positive class)\n",
    "classes = y.unique()\n",
    "neg_class_label = False # Or the actual label for the negative class if not boolean\n",
    "pos_class_label = True # Or the actual label for the positive class if not boolean\n",
    "\n",
    "if neg_class_label in y.value_counts() and pos_class_label in y.value_counts():\n",
    "    neg_count = y.value_counts()[neg_class_label]\n",
    "    pos_count = y.value_counts()[pos_class_label]\n",
    "    scale_pos_weight_value = neg_count / pos_count\n",
    "    print(f\"Calculated scale_pos_weight: {scale_pos_weight_value:.4f}\")\n",
    "else:\n",
    "     print(\"Warning: Could not find expected negative or positive class labels in y.\")\n",
    "     scale_pos_weight_value = 1.0 # Default to 1 if labels not found\n",
    "\n",
    "models = [{\"name\": \"Logistic Regression\",\n",
    "           \"model\": LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced', n_jobs=-1)},\n",
    "          {\"name\": \"Random Forest\",\n",
    "           \"model\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)},\n",
    "          {\"name\": \"XGBoost\",\n",
    "           \"model\": xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False,\n",
    "                                      random_state=42, n_jobs=-1, scale_pos_weight=scale_pos_weight_value)},\n",
    "          {\"name\": \"LightGBM\",\n",
    "           \"model\": lgb.LGBMClassifier(objective='binary', metric='binary_logloss', random_state=42, n_jobs=-1,\n",
    "                                       scale_pos_weight=scale_pos_weight_value)}\n",
    "  ]\n",
    "\n",
    "result = {}\n",
    "\n",
    "for model_info in models:\n",
    "    print(f\"\\n--- Training and Evaluating: {model_info['name']} ---\")\n",
    "    current_pipeline, metrics = train_evaluate_and_get_metrics_without_smote(model_info['model'], X, y)\n",
    "\n",
    "    result[model_info['name']] = metrics\n",
    "    print(f\"Metrics stored: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf439174",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lxxa02vWMOaL",
    "outputId": "b08f3b6f-51bb-4417-c8a7-1f6dc5f4f233",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_2nd = pd.DataFrame(result); display(result_2nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f664782",
   "metadata": {
    "id": "_RSkGIEaMWct",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "세번째 학습\n",
    "- features_leakage_related + features_highly_correlated 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e7851",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGPoenJIMY4j",
    "outputId": "832fc3c2-f8c1-4d90-cbef-dd4576f0e76f",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(['is_default'] + features_leakage_related + features_highly_correlated, axis=1)\n",
    "y = df['is_default']\n",
    "\n",
    "result = {}\n",
    "\n",
    "for model_info in models:\n",
    "    print(f\"\\n--- Training and Evaluating: {model_info['name']} ---\")\n",
    "    current_pipeline, metrics = train_evaluate_and_get_metrics_without_smote(model_info['model'], X, y)\n",
    "\n",
    "    result[model_info['name']] = metrics\n",
    "    print(f\"Metrics stored: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231f4ef4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-irWxt_UQR3n",
    "outputId": "79861766-3edf-4ebe-faa7-27ea8559a708",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_3rd = pd.DataFrame(result); display(result_3rd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1680da",
   "metadata": {
    "id": "OdZ0AMEtIprN",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "네번쨰 학습\n",
    "- features_leakage_related + features_highly_correlated + features_to_check 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309257fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "SE2piGEjClwD",
    "outputId": "7def42d9-70ae-43ea-e0b4-40843b3bd2b2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(['is_default'] + features_leakage_related + features_highly_correlated + features_to_check, axis=1)\n",
    "y = df['is_default']\n",
    "\n",
    "result = {}\n",
    "\n",
    "for model_info in models:\n",
    "    print(f\"\\n--- Training and Evaluating: {model_info['name']} ---\")\n",
    "    current_pipeline, metrics = train_evaluate_and_get_metrics_without_smote(model_info['model'], X, y)\n",
    "\n",
    "    result[model_info['name']] = metrics\n",
    "    print(f\"Metrics stored: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2eb319",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "g_I6erxpFpZU",
    "outputId": "800c9cf2-c9c2-41f1-c25e-0bfb4dc5b1ef",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NO SMOTE\n",
    "# features_leakage_related + features_highly_correlated + features_to_check 모두 제외\n",
    "result_4th = pd.DataFrame(result); display(result_4th)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175752ef",
   "metadata": {
    "id": "bjLWcOXjRUdB",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "- Feature Importance Plot을 통해 특징별 Importance가 이전 학습결과와 다르게 비교적 고르게 분포하고 있음을 확인하였습니다.\n",
    "\n",
    "- 분류 성능도 `True`( `Default` )에 대한 Recall 과 F1-score 를 봤을 때, 전형적인 클래스 불균형 문제를 가진 분류 문제에서의 성능이 도출되었습니다.\n",
    "\n",
    "- 따라서, 데이터 누수 문제는 해결되었다고 판단하고 분류 성능 향상을 위한 추가 작업을 진행하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8409f430",
   "metadata": {
    "id": "tpRI_fTKRRW5",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf619a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5o0x__gRQ5Z",
    "outputId": "64b83c4c-f8bc-4e04-f477-e083a63bbd93",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"두번째 학습 결과: features_leakage_related + features_to_check 제외\")\n",
    "print(result_2nd)\n",
    "print(\"\\n세번째 학습 결과: features_leakage_related + features_highly_correlated 제외\")\n",
    "print(result_3rd)\n",
    "print(\"\\n네번째 학습 결과: features_leakage_related + features_highly_correlated + features_to_check 제외\")\n",
    "print(result_4th)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb442d",
   "metadata": {
    "id": "SQKpphJVSrzy",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 교차검증\n",
    "-  첫번째 학습:  features_leakage_related + features_to_check 제외\n",
    "-  두번째 학습: features_leakage_related + features_highly_correlated 제외\n",
    "- 세번째 학습 결과: features_leakage_related + features_highly_correlated + features_to_check 제외\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c6347",
   "metadata": {
    "id": "jILDa-JyTd9s",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### custom 함수\n",
    "- evaluate_model_with_cross_validation(model_pipeline, X, y, cv, scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8853d9",
   "metadata": {
    "id": "yewqqXeQSsq_",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model_with_cross_validation(model_pipeline, X, y, cv, scoring):\n",
    "    \"\"\"\n",
    "    Evaluates a machine learning model pipeline using cross-validation.\n",
    "\n",
    "    Args:\n",
    "        model_pipeline: The machine learning model pipeline to evaluate.\n",
    "        X: The feature DataFrame.\n",
    "        y: The target Series.\n",
    "        cv: Cross-validation splitter (e.g., StratifiedKFold).\n",
    "        scoring: A dictionary of scoring metrics.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the cross-validation results.\n",
    "    \"\"\"\n",
    "    print(\"Performing cross-validation...\")\n",
    "\n",
    "    cv_results = cross_validate(\n",
    "        estimator=model_pipeline,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        return_train_score=False, # We only need test scores for evaluation\n",
    "        n_jobs=-1 # Use all available cores\n",
    "    )\n",
    "    print(\"Cross-validation complete.\")\n",
    "    return cv_results\n",
    "\n",
    "# Define scoring metrics for cross-validation\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, pos_label=True),\n",
    "    'recall': make_scorer(recall_score, pos_label=True),\n",
    "    'f1_score': make_scorer(f1_score, pos_label=True),\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True) # AUC requires predict_proba\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3264fad0",
   "metadata": {
    "id": "MEKaeraTTkkZ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 첫번째 학습\n",
    "- features_leakage_related + features_to_check 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d7782",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "vVoQVnVfTkXP",
    "outputId": "5435c1da-d01b-409a-f29c-7e0bf162210d",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(['is_default'] + features_leakage_related + features_to_check, axis=1)\n",
    "y = df['is_default']\n",
    "\n",
    "# 교차 검증에 사용할 모델 정의\n",
    "models = [{\"name\": \"Logistic Regression\",\n",
    "           \"model\": LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced', n_jobs=-1)},\n",
    "          {\"name\": \"Random Forest\",\n",
    "           \"model\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)},\n",
    "          {\"name\": \"XGBoost\",\n",
    "           \"model\": xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False,\n",
    "                                      random_state=42, n_jobs=-1, scale_pos_weight=scale_pos_weight_value)},\n",
    "          {\"name\": \"LightGBM\",\n",
    "           \"model\": lgb.LGBMClassifier(objective='binary', metric='binary_logloss', random_state=42, n_jobs=-1,\n",
    "                                       scale_pos_weight=scale_pos_weight_value)}\n",
    "  ]\n",
    "\n",
    "# 교차 검증을 위한 파이프라인 정의\n",
    "categorical_features = X.select_dtypes('object').columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "features_to_log_transform = [\n",
    "    'int_rate', 'installment', 'annual_inc', 'dti', 'open_acc',\n",
    "    'revol_bal', 'total_acc', 'total_rec_int', 'mths_since_rcnt_il', 'il_util',\n",
    "    'max_bal_bc', 'total_rev_hi_lim', 'avg_cur_bal', 'bc_open_to_buy'\n",
    "]\n",
    "numerical_log_features = [f for f in numerical_features if f in features_to_log_transform]\n",
    "numerical_scale_only_features = [f for f in numerical_features if f not in features_to_log_transform]\n",
    "\n",
    "log_transformer = FunctionTransformer(func=np.log1p, validate=False)\n",
    "numerical_scaler = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_log', Pipeline([('log', log_transformer), ('scaler', numerical_scaler)]), numerical_log_features),\n",
    "        ('num_scale', numerical_scaler, numerical_scale_only_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = {}\n",
    "for model_info in models:\n",
    "    print(f\"\\n--- Cross-validation: {model_info['name']} ---\")\n",
    "\n",
    "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', model_info['model'])])\n",
    "\n",
    "    cv_results[model_info['name']] = evaluate_model_with_cross_validation(model_pipeline, X, y, cv, scoring)\n",
    "    print(\"\\nCross-validation results:\")\n",
    "    display(pd.DataFrame(cv_results[model_info['name']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a852c0",
   "metadata": {
    "id": "ong0cXQmXjhU",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Cross-validation Summary (Mean Metrics) ---\")\n",
    "mean_cv_results = {}\n",
    "for model_name, results in cv_results.items():\n",
    "    mean_cv_results[model_name] = {metric: np.mean(scores) for metric, scores in results.items()}\n",
    "display(pd.DataFrame(mean_cv_results).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2bfbe",
   "metadata": {
    "id": "IyoVwExrW_0H",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 두번째 학습\n",
    "- features_leakage_related + features_highly_correlated 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519537ef",
   "metadata": {
    "id": "-x5h__BDXUcp",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(['is_default'] + features_leakage_related + features_highly_correlated, axis=1)\n",
    "y = df['is_default']\n",
    "\n",
    "\n",
    "# 교차 검증을 위한 파이프라인 정의\n",
    "categorical_features = X.select_dtypes('object').columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "features_to_log_transform = [\n",
    "    'int_rate', 'installment', 'annual_inc', 'dti', 'open_acc',\n",
    "    'revol_bal', 'total_acc', 'total_rec_int', 'mths_since_rcnt_il', 'il_util',\n",
    "    'max_bal_bc', 'total_rev_hi_lim', 'avg_cur_bal', 'bc_open_to_buy'\n",
    "]\n",
    "numerical_log_features = [f for f in numerical_features if f in features_to_log_transform]\n",
    "numerical_scale_only_features = [f for f in numerical_features if f not in features_to_log_transform]\n",
    "\n",
    "log_transformer = FunctionTransformer(func=np.log1p, validate=False)\n",
    "numerical_scaler = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_log', Pipeline([('log', log_transformer), ('scaler', numerical_scaler)]), numerical_log_features),\n",
    "        ('num_scale', numerical_scaler, numerical_scale_only_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = {}\n",
    "for model_info in models:\n",
    "    print(f\"\\n--- Cross-validation: {model_info['name']} ---\")\n",
    "\n",
    "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', model_info['model'])])\n",
    "\n",
    "    cv_results[model_info['name']] = evaluate_model_with_cross_validation(model_pipeline, X, y, cv, scoring)\n",
    "    print(\"\\nCross-validation results:\")\n",
    "    display(pd.DataFrame(cv_results[model_info['name']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1bdad0",
   "metadata": {
    "id": "Z9XJnwdnXoBl",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 상관 분석\n",
    "- 강건한 분류 성능과 다중공선성 해결 위해 0.9 이상의 강한 상관 관계를 가지는 특징에 대한 전처리를 수행하였습니다.\n",
    "- 삭제할 특징 선택 기준은 이전 학습 모델의 Feature Importance 를 활용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f5b2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T18:14:37.454606Z",
     "iopub.status.busy": "2025-09-18T18:14:37.454193Z",
     "iopub.status.idle": "2025-09-18T18:14:37.490525Z",
     "shell.execute_reply": "2025-09-18T18:14:37.488993Z",
     "shell.execute_reply.started": "2025-09-18T18:14:37.454573Z"
    },
    "id": "tttQZzrLgGCu",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unstacked_corr_matrix(df):\n",
    "  # 상관계수가 0.5 이상인 특징만 필터링\n",
    "  corr_matrix = df.select_dtypes(['number', 'bool']).corr()\n",
    "\n",
    "  stacked_corr = corr_matrix.unstack().reset_index()\n",
    "  stacked_corr.columns = ['feature1', 'feature2', 'correlation']\n",
    "\n",
    "  filtered_corr = stacked_corr[stacked_corr['feature1'] > stacked_corr['feature2']]\n",
    "  high_corr_pairs = filtered_corr[abs(filtered_corr['correlation']) > 0.5]\n",
    "\n",
    "  high_corr_pairs = high_corr_pairs.sort_values(by='correlation', ascending=False)\n",
    "\n",
    "  return high_corr_pairs\n",
    "\n",
    "def prep_corrlated_feautres(corr_matrix):\n",
    "  # 상관계수가 0.9 이상인 특징 목록 추출\n",
    "  filtered_matrix = corr_matrix[abs(corr_matrix['correlation'] >= 0.9)]\n",
    "\n",
    "  feature_list = []\n",
    "  feature_list.append(filtered_matrix['feature1'].values.tolist())\n",
    "  feature_list.append(filtered_matrix['feature2'].values.tolist())\n",
    "\n",
    "  return np.unique(np.array(feature_list).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d9e30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T18:14:37.492018Z",
     "iopub.status.busy": "2025-09-18T18:14:37.491729Z",
     "iopub.status.idle": "2025-09-18T18:15:12.659317Z",
     "shell.execute_reply": "2025-09-18T18:15:12.658280Z",
     "shell.execute_reply.started": "2025-09-18T18:14:37.491989Z"
    },
    "id": "dyVWJOfN9Suq",
    "outputId": "6980e095-2a2e-4dbe-a1ba-58e420d804e4",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# is_default 와의 상관관계\n",
    "corr_matrix = df.select_dtypes(['number', 'bool']).corr()\n",
    "\n",
    "abs(corr_matrix['is_default']).sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ced1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T18:15:12.660728Z",
     "iopub.status.busy": "2025-09-18T18:15:12.660444Z",
     "iopub.status.idle": "2025-09-18T18:15:40.203690Z",
     "shell.execute_reply": "2025-09-18T18:15:40.202505Z",
     "shell.execute_reply.started": "2025-09-18T18:15:12.660705Z"
    },
    "id": "8bX8PRvpTdqY",
    "outputId": "c1c6e568-58ce-43d0-d8bd-b4976d9c2d49",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 특징 간 상관관계\n",
    "high_corr_pairs = get_unstacked_corr_matrix(X)\n",
    "print(high_corr_pairs[abs(high_corr_pairs['correlation'] >= 0.9)])\n",
    "\n",
    "# --- 결과 기록 ---\n",
    "#                         feature1               feature2  correlation\n",
    "# 821               fico_range_low        fico_range_high     1.000000\n",
    "# 1                      loan_amnt            funded_amnt     0.999999\n",
    "# 163              funded_amnt_inv            funded_amnt     0.999995\n",
    "# 2                      loan_amnt        funded_amnt_inv     0.999994\n",
    "# 1115                    open_acc               num_sats     0.999096\n",
    "# 4996         num_rev_tl_bal_gt_0        num_actv_rev_tl     0.982402\n",
    "# 5775             tot_hi_cred_lim            tot_cur_bal     0.974933\n",
    "# 6024  total_il_high_credit_limit           total_bal_il     0.952069\n",
    "# 406                  installment            funded_amnt     0.945982\n",
    "# 5                      loan_amnt            installment     0.945981\n",
    "# 407                  installment        funded_amnt_inv     0.945880\n",
    "# 3885        mo_sin_old_rev_tl_op  credit_history_length     0.912267\n",
    "# 2502                total_bal_il      total_bal_ex_mort     0.900444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b83117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T18:15:40.208942Z",
     "iopub.status.busy": "2025-09-18T18:15:40.208613Z",
     "iopub.status.idle": "2025-09-18T18:15:40.214528Z",
     "shell.execute_reply": "2025-09-18T18:15:40.213277Z",
     "shell.execute_reply.started": "2025-09-18T18:15:40.208922Z"
    },
    "id": "SqqZTJPzbNOC",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature importance based feature selection\n",
    "# 상관계수가 0.9 이상인 특징 쌍 중 feature importance가 낮은 특징을 삭제 처리\n",
    "\n",
    "# feature_list = prep_corrlated_feautres(high_corr_pairs)\n",
    "\n",
    "# print(feature_importance[[col for col in feature_importance.index if any(keyword in col for keyword in feature_list)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae9dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T01:15:31.877264Z",
     "iopub.status.busy": "2025-09-19T01:15:31.876889Z",
     "iopub.status.idle": "2025-09-19T01:15:31.882316Z",
     "shell.execute_reply": "2025-09-19T01:15:31.881371Z",
     "shell.execute_reply.started": "2025-09-19T01:15:31.877236Z"
    },
    "id": "SYWZUH-gh8nP",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 삭제대상\n",
    "# features_to_be_deleted = ['fico_range_low', 'loan_amnt', 'funded_amnt_inv', 'funded_amnt', 'num_sats',\n",
    "#                           'num_rev_tl_bal_gt_0', 'tot_cur_bal', 'total_bal_il', 'mo_sin_old_rev_tl_op']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee4db45",
   "metadata": {
    "id": "SfgwAk9dfAbA",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 모델 학습 with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56654b03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T01:12:00.894521Z",
     "iopub.status.busy": "2025-09-19T01:12:00.892233Z",
     "iopub.status.idle": "2025-09-19T01:12:01.555967Z",
     "shell.execute_reply": "2025-09-19T01:12:01.554908Z",
     "shell.execute_reply.started": "2025-09-19T01:12:00.894479Z"
    },
    "id": "r6FOlW2GkJaT",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "def train_and_evaluate_model_using_SMOTE(model, X, y):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a machine learning model with SMOTE oversampling\n",
    "    using imblearn Pipeline.\n",
    "\n",
    "    Args:\n",
    "        model: The machine learning model to train.\n",
    "        X: The feature DataFrame.\n",
    "        y: The target Series.\n",
    "\n",
    "    Returns:\n",
    "        The trained model pipeline.\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Use stratify\n",
    "\n",
    "    categorical_features = X_train.select_dtypes('object').columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Create a column transformer for preprocessing\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Create an imblearn pipeline\n",
    "    smote = SMOTE(random_state=42)\n",
    "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                     ('smote', smote), # Add SMOTE step\n",
    "                                     ('classifier', model)]) # Use the input model here\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nConfusion Matrix :\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d481ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:10.085439Z",
     "iopub.status.busy": "2025-09-18T18:31:10.084927Z",
     "iopub.status.idle": "2025-09-18T18:31:10.999222Z",
     "shell.execute_reply": "2025-09-18T18:31:10.997834Z",
     "shell.execute_reply.started": "2025-09-18T18:31:10.085409Z"
    },
    "id": "trl2SA__57A_",
    "outputId": "199fd690-d213-48d1-aa55-c3f7adbcef04",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature importance\n",
    "# feature_importance = plot_feature_importance(model_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64f5a73",
   "metadata": {
    "id": "HBk7EPn5ETge",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "- SMOTE를 수행헀음에도 Default에 대한 분류 성능을 봤을 때, 데이터 전처리가 더 필요하다고 판단됩니다.\n",
    "- 수치형 특징에 대한 데이터 탐색을 후 적절한 추가 전처리를 수행한 후 다시 학습을 진행하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137d939a",
   "metadata": {
    "id": "AUu0YZFfFIO6",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 데이터 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521eccbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:11.000803Z",
     "iopub.status.busy": "2025-09-18T18:31:11.000511Z",
     "iopub.status.idle": "2025-09-18T18:31:15.787714Z",
     "shell.execute_reply": "2025-09-18T18:31:15.786645Z",
     "shell.execute_reply.started": "2025-09-18T18:31:11.000781Z"
    },
    "id": "EYySgwGODMGR",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_to_be_deleted = ['fico_range_low', 'loan_amnt', 'funded_amnt_inv', 'funded_amnt', 'num_sats',\n",
    "                          'num_rev_tl_bal_gt_0', 'tot_cur_bal', 'total_bal_il', 'mo_sin_old_rev_tl_op']\n",
    "\n",
    "X = df.drop([\n",
    "    'is_default',\n",
    "    'last_fico_range_high', 'last_fico_range_low',\n",
    "    'recoveries', 'collection_recovery_fee',\n",
    "    'total_rec_prncp', 'out_prncp_inv', 'total_pymnt_inv',\n",
    "    'last_pymnt_amnt', 'out_prncp', 'total_pymnt'] + features_to_be_deleted, axis=1)\n",
    "y = df['is_default']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Use stratify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b002cd",
   "metadata": {
    "id": "1zcrPKIWoDna",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Int 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2307755",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:15.789299Z",
     "iopub.status.busy": "2025-09-18T18:31:15.788905Z",
     "iopub.status.idle": "2025-09-18T18:31:16.650429Z",
     "shell.execute_reply": "2025-09-18T18:31:16.649445Z",
     "shell.execute_reply.started": "2025-09-18T18:31:15.789267Z"
    },
    "id": "5BkJs1gAFiug",
    "outputId": "3ce92c66-dc88-4693-97d2-ecd4252b6f7b",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "int_features = X_train.select_dtypes('int').columns\n",
    "float_features = X_train.select_dtypes('float').columns\n",
    "\n",
    "len(int_features), len(float_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4e109",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:16.651637Z",
     "iopub.status.busy": "2025-09-18T18:31:16.651319Z",
     "iopub.status.idle": "2025-09-18T18:31:16.839651Z",
     "shell.execute_reply": "2025-09-18T18:31:16.838579Z",
     "shell.execute_reply.started": "2025-09-18T18:31:16.651616Z"
    },
    "id": "f9TdUn5dFnvb",
    "outputId": "939af3fe-7e57-4f9f-ef6f-2b9790005e28",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "X_train[int_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41d033",
   "metadata": {
    "id": "DxXGPbpcoPyH",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "- 정수 타입 특징 중 `time_since_last_credit_check`의 기술통계량을 보고 한쪽으로 치우친 분포를 가지고 있다고 판단하여 시각화를 진행하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4defd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:16.840998Z",
     "iopub.status.busy": "2025-09-18T18:31:16.840634Z",
     "iopub.status.idle": "2025-09-18T18:31:18.628022Z",
     "shell.execute_reply": "2025-09-18T18:31:18.626904Z",
     "shell.execute_reply.started": "2025-09-18T18:31:16.840965Z"
    },
    "id": "eShjOgwfH18F",
    "outputId": "c51d754b-bbdb-4e48-f345-5ef8976b27b5",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "sns.histplot(X_train['time_since_last_credit_check'])\n",
    "plt.title('Original Distribution of time_since_last_credit_check')\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.histplot(np.log1p(X_train['time_since_last_credit_check']))\n",
    "plt.title('Log-transformed Distribution of time_since_last_credit_check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18eda8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:18.629542Z",
     "iopub.status.busy": "2025-09-18T18:31:18.629139Z",
     "iopub.status.idle": "2025-09-18T18:31:19.036530Z",
     "shell.execute_reply": "2025-09-18T18:31:19.035396Z",
     "shell.execute_reply.started": "2025-09-18T18:31:18.629511Z"
    },
    "id": "c9df8dab",
    "outputId": "3de7e1af-a7b2-4a77-cedc-7942936d44e1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.countplot(data=df, x='time_since_last_credit_check', hue='is_default', palette='viridis')\n",
    "plt.title('Distribution of time_since_last_credit_check by is_default')\n",
    "plt.xlabel('Time Since Last Credit Check')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cecadc",
   "metadata": {
    "id": "lWMS86lxoMfl",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "- `time_since_last_credit_check`는 한쪽으로 크게 기울어진 분포를 가진 특징이지만 타겟 특징과의 분포 확인 결과, 분류에 유의미할 것으로 판단하였습니다.\n",
    "- 로그변환은 하지 않았습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9beaf4a",
   "metadata": {
    "id": "3cfd7db8",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Float 타입\n",
    "- 전처리된 데이터 기준으로 총 67개로 구성되며, 모두 분포를 시각화하며 확인할 수 없기 때문에 기술통계량을 기준으로 필터링 후 시각화를 통해 분포를 확인하였습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb5646",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:19.038057Z",
     "iopub.status.busy": "2025-09-18T18:31:19.037690Z",
     "iopub.status.idle": "2025-09-18T18:31:23.080226Z",
     "shell.execute_reply": "2025-09-18T18:31:23.079422Z",
     "shell.execute_reply.started": "2025-09-18T18:31:19.038025Z"
    },
    "id": "e8468740",
    "outputId": "09ede05e-9505-43cd-9148-808bbab5f0f6",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "float_features = X_train.select_dtypes('float').columns\n",
    "display(X_train[float_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9629236",
   "metadata": {
    "id": "071101f4",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "분포-불균형 특징 식별\n",
    "- 기술 통계량 결과를 바탕으로, 평균 및 표준편차와 비교했을 때 최대/최소 값의 차이가 큰 특징들을 식별하였습니다.\n",
    "- 기준 : `max > mean + 3*std` 또는 `min < mean - 3*std`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56b0e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:23.082415Z",
     "iopub.status.busy": "2025-09-18T18:31:23.081514Z",
     "iopub.status.idle": "2025-09-18T18:31:23.090515Z",
     "shell.execute_reply": "2025-09-18T18:31:23.089480Z",
     "shell.execute_reply.started": "2025-09-18T18:31:23.082364Z"
    },
    "id": "f35ea2ed",
    "outputId": "0b516c78-a4ad-4ff7-baa1-ca9c1b57d0c9",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze descriptive statistics to identify features with potential outliers or skewed distributions\n",
    "\n",
    "float_features_desc = X_train[float_features].describe()\n",
    "\n",
    "potential_outlier_features = []\n",
    "\n",
    "for col in float_features_desc.columns:\n",
    "    mean_val = float_features_desc.loc['mean', col]\n",
    "    std_val = float_features_desc.loc['std', col]\n",
    "    min_val = float_features_desc.loc['min', col]\n",
    "    max_val = float_features_desc.loc['max', col]\n",
    "\n",
    "    # Check for extreme maximum values\n",
    "    if max_val > mean_val + 3 * std_val:\n",
    "        potential_outlier_features.append(col)\n",
    "        print(f\"'{col}': Max value ({max_val:.4f}) is significantly larger than mean ({mean_val:.4f}) + 3*std ({3 * std_val:.4f})\")\n",
    "\n",
    "    # Check for extreme minimum values\n",
    "    if min_val < mean_val - 3 * std_val:\n",
    "        potential_outlier_features.append(col)\n",
    "        print(f\"'{col}': Min value ({min_val:.4f}) is significantly smaller than mean ({mean_val:.4f}) - 3*std ({3 * std_val:.4f})\")\n",
    "\n",
    "# Remove duplicates from the list\n",
    "potential_outlier_features = list(set(potential_outlier_features))\n",
    "\n",
    "print(\"\\nFeatures identified with potential extreme values:\")\n",
    "print(potential_outlier_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f926722",
   "metadata": {
    "id": "593d20ab",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "##분포-불균형 의심특징 분포 시각화\n",
    "\n",
    "- 식별된 특징들에 대해 히스토그램 또는 상자 그림(boxplot)을 그려 분포를 시각적으로 확인하였습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99d12b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:23.091978Z",
     "iopub.status.busy": "2025-09-18T18:31:23.091705Z",
     "iopub.status.idle": "2025-09-18T18:31:23.119712Z",
     "shell.execute_reply": "2025-09-18T18:31:23.118293Z",
     "shell.execute_reply.started": "2025-09-18T18:31:23.091957Z"
    },
    "id": "44e48fd1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for col in potential_outlier_features:\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     sns.histplot(X_train[col], kde=True)\n",
    "#     plt.title(f'Original Distribution of {col}')\n",
    "\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     sns.histplot(np.log1p(X_train[col]), kde=True)\n",
    "#     plt.title(f'Log1p Transformed Distribution of {col}')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec4f10",
   "metadata": {
    "id": "G-htJYS7OdXb",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "1.로그 변환이 필요한 특징\n",
    "- `int_rate`, `installment`, `annual_inc`, `dti_joint`, `dti`, `open_acc`, `revol_bal`, `total_acc`, `total_rec_int`, `mths_since_rcnt_il`, `il_util`, `max_bal_bc`, `total_rev_hi_lim`, `avg_cur_bal`, `bc_open_to_buy`, `tot_coll_amt`,\n",
    "\n",
    "2.정보량 부족한 특징\n",
    "- target 과의 분포 확인하고 처리\n",
    "- 대상: `delinq_2yrs`, `fico_range_high`, `inq_last_6mths`, `pub_rec`, `total_rec_late_fee`, `collections_12_mths_ex_med`, `acc_now_delinq`, `chargeoff_within_12_mhts`, `delinq_amnt`\n",
    "\n",
    "3.int 변환이 필요해보이는 특징\n",
    "  - `open_acc_6m`, `open_act_il`, `open_il_12m`, `open_il_24m`, `open_rv_12m`, `open_rv_24m`, `inq_fi`, `total_cu_tl`, `inq_last_12m`\n",
    "\n",
    "4.변환 필요 없음\n",
    "- 대상: `revol_util`, `all_util`, `bc_util`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513214a",
   "metadata": {
    "id": "oFd4UfN4sLtG",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 추가 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e76751",
   "metadata": {
    "id": "t3dF1zMsQaCj",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "로그변환은 함수화하여 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f9379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:23.121118Z",
     "iopub.status.busy": "2025-09-18T18:31:23.120764Z",
     "iopub.status.idle": "2025-09-18T18:31:23.147809Z",
     "shell.execute_reply": "2025-09-18T18:31:23.146469Z",
     "shell.execute_reply.started": "2025-09-18T18:31:23.121087Z"
    },
    "id": "XbxemUzFQ6kp",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 로그변환 대상 특징\n",
    "# features_to_log_transform = [\n",
    "#     'int_rate', 'installment', 'annual_inc', 'dti', 'open_acc',\n",
    "#     'revol_bal', 'total_acc', 'total_rec_int', 'mths_since_rcnt_il', 'il_util',\n",
    "#     'max_bal_bc', 'total_rev_hi_lim', 'avg_cur_bal', 'bc_open_to_buy', 'tot_coll_amt'\n",
    "# ]\n",
    "\n",
    "# for feature in features_to_log_transform:\n",
    "#     if feature in X_train.columns:\n",
    "#         X_train[feature] = np.log1p(X_train[feature])\n",
    "#         # Apply the same transformation to the test set\n",
    "#         if feature in X_test.columns:\n",
    "#             X_test[feature] = np.log1p(X_test[feature])\n",
    "#     else:\n",
    "#         print(f\"Warning: Feature '{feature}' not found in X_train.\")\n",
    "\n",
    "# print(\"로그 변환 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6adb84f",
   "metadata": {
    "id": "838kuoZ0RYmf",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "정보량 부족한 특징\n",
    "- target과의 분포 확인 후 조치하였습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32b498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:23.149872Z",
     "iopub.status.busy": "2025-09-18T18:31:23.148945Z",
     "iopub.status.idle": "2025-09-18T18:31:23.168249Z",
     "shell.execute_reply": "2025-09-18T18:31:23.166937Z",
     "shell.execute_reply.started": "2025-09-18T18:31:23.149843Z"
    },
    "id": "GXLxUbwCRe5x",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 대상 특징\n",
    "# features_to_check = [\n",
    "#     'delinq_2yrs', 'fico_range_high', 'inq_last_6mths', 'pub_rec',\n",
    "#     'total_rec_late_fee', 'collections_12_mths_ex_med', 'acc_now_delinq', 'chargeoff_within_12_mths', 'delinq_amnt'\n",
    "# ]\n",
    "\n",
    "# # Determine plot type based on the number of unique values\n",
    "# def plot_feature_distribution_by_target(df, feature, target='is_default'):\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     if df[feature].nunique() < 20: # Use countplot for features with few unique values\n",
    "#         sns.countplot(data=df, x=feature, hue=target, palette='viridis')\n",
    "#         plt.title(f'Distribution of {feature} by {target}')\n",
    "#         plt.xticks(rotation=45)\n",
    "\n",
    "#     else: # Use histplot for features with many unique values\n",
    "#         sns.histplot(data=df, x=feature, hue=target, kde=True, palette='viridis', common_norm=False)\n",
    "#         plt.title(f'Distribution of {feature} by {target}')\n",
    "\n",
    "#     plt.xlabel(feature)\n",
    "#     plt.ylabel('Count' if df[feature].nunique() < 20 else 'Density')\n",
    "#     plt.show()\n",
    "\n",
    "# # Plot distributions for the features to check\n",
    "# for feature in features_to_check:\n",
    "#     if feature in df.columns:\n",
    "#         plot_feature_distribution_by_target(df, feature)\n",
    "#     else:\n",
    "#         print(f\"Warning: Feature '{feature}' not found in DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5038b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:23.170123Z",
     "iopub.status.busy": "2025-09-18T18:31:23.169857Z",
     "iopub.status.idle": "2025-09-18T18:31:23.195442Z",
     "shell.execute_reply": "2025-09-18T18:31:23.193919Z",
     "shell.execute_reply.started": "2025-09-18T18:31:23.170103Z"
    },
    "id": "NuX-ikmNuN5v",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features_to_check의 feature importance 확인\n",
    "# print(feature_importance[[col for col in feature_importance.index if any(keyword in col for keyword in features_to_check)]])\n",
    "\n",
    "# num__total_rec_late_fee           0.0358\n",
    "# num__inq_last_6mths               0.0156\n",
    "# num__fico_range_high              0.0089\n",
    "# num__delinq_2yrs                  0.0025\n",
    "# num__pub_rec                      0.0012\n",
    "# num__pub_rec_bankruptcies         0.0009\n",
    "# num__collections_12_mths_ex_med   0.0003\n",
    "# num__chargeoff_within_12_mths     0.0002\n",
    "# num__delinq_amnt                  0.0001\n",
    "# num__acc_now_delinq               0.0001\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b578efc",
   "metadata": {
    "id": "3k0bfHyPU59m",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "- 타겟 특징과의 분포도 확인해봤지만 유의미한 특징으로 보이지 않음\n",
    "- 따라서 해당 특징들은 학습에서 제외하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d8b62a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:23.196860Z",
     "iopub.status.busy": "2025-09-18T18:31:23.196563Z",
     "iopub.status.idle": "2025-09-18T18:31:23.214634Z",
     "shell.execute_reply": "2025-09-18T18:31:23.213444Z",
     "shell.execute_reply.started": "2025-09-18T18:31:23.196833Z"
    },
    "id": "Ozr9YFvfTGTg",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features_to_check = [\n",
    "#     'delinq_2yrs', 'fico_range_high', 'inq_last_6mths', 'pub_rec',\n",
    "#     'total_rec_late_fee', 'collections_12_mths_ex_med', 'acc_now_delinq', 'chargeoff_within_12_mths', 'delinq_amnt'\n",
    "# ]\n",
    "# features_to_be_deleted = features_to_be_deleted + features_to_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4bc0c",
   "metadata": {
    "id": "RqbffjBdvY3G",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "int 변환이 필요해보이는 특징\n",
    "- 이 특징들은 Float type 이지만 int type 특징처럼 bar 분포를 가지고 있었습니다.\n",
    "- 이에, 타겟 변수와의 분포 확인 후 유의미하다면, binning 후 범주형 특징으로 변환하면 좋을 것 같습니다.\n",
    "\n",
    "- `open_acc_6m`, `open_act_il`, `open_il_12m`, `open_il_24m`, `open_rv_12m`, `open_rv_24m`, `inq_fi`, `total_cu_tl`, `inq_last_12m`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3609107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:23.216212Z",
     "iopub.status.busy": "2025-09-18T18:31:23.215930Z",
     "iopub.status.idle": "2025-09-18T18:31:23.235512Z",
     "shell.execute_reply": "2025-09-18T18:31:23.234326Z",
     "shell.execute_reply.started": "2025-09-18T18:31:23.216189Z"
    },
    "id": "f9db84ea",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 정수형에 가까운 float 특징 목록\n",
    "# features_to_check_2nd = [\n",
    "#     'open_acc_6m', 'open_act_il', 'open_il_12m', 'open_il_24m',\n",
    "#     'open_rv_12m', 'open_rv_24m',\n",
    "#     'inq_fi', 'total_cu_tl', 'inq_last_12m',\n",
    "# ]\n",
    "\n",
    "# # Calculate and plot the proportion of 'Default' for each feature value\n",
    "# for feature in features_to_check_2nd:\n",
    "#     if feature in df.columns:\n",
    "#         # Calculate proportions of is_default for each unique value of the feature\n",
    "#         # Group by the feature and then calculate the mean of 'is_default' (True=1, False=0)\n",
    "#         default_proportion = df.groupby(feature)['is_default'].mean().reset_index()\n",
    "#         default_proportion.columns = [feature, 'default_proportion']\n",
    "\n",
    "#         # Plot the proportions\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         # Modified sns.barplot call to address FutureWarning\n",
    "#         sns.barplot(data=default_proportion, x=feature, y='default_proportion', hue=feature, palette='viridis', legend=False)\n",
    "#         plt.title(f'Proportion of Default by {feature}')\n",
    "#         plt.xlabel(feature)\n",
    "#         plt.ylabel('Proportion of Default')\n",
    "#         plt.xticks(rotation=45) # Rotate labels for better readability\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         print(f\"Warning: Feature '{feature}' not found in DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d83e466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T18:31:23.237869Z",
     "iopub.status.busy": "2025-09-18T18:31:23.237478Z",
     "iopub.status.idle": "2025-09-18T18:31:23.262002Z",
     "shell.execute_reply": "2025-09-18T18:31:23.260631Z",
     "shell.execute_reply.started": "2025-09-18T18:31:23.237836Z"
    },
    "id": "TXtxEgeWwthM",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(feature_importance[[col for col in feature_importance.index if any(keyword in col for keyword in features_to_check_2nd)]])\n",
    "\n",
    "# num__open_rv_24m    0.0179\n",
    "# num__inq_fi         0.0173\n",
    "# num__inq_last_12m   0.0168\n",
    "# num__total_cu_tl    0.0144\n",
    "# num__open_rv_12m    0.0106\n",
    "# num__open_acc_6m    0.0089\n",
    "# num__open_il_24m    0.0070\n",
    "# num__open_act_il    0.0055\n",
    "# num__open_il_12m    0.0045\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32610b91",
   "metadata": {
    "id": "Zr36nwhTw3tL",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "이 특징들 또한 학습에서 제외하는 것이 맞다고 판단하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad63198f",
   "metadata": {
    "id": "weUNUaA6Vzmp",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 모델 재학습\n",
    "데이터 탐색 및 피처 엔지니어링을 적용한 후 모델 학습을 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3345a1",
   "metadata": {
    "id": "xLcqJdFo06uC",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 첫번째 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f9b9c",
   "metadata": {
    "id": "ZAj6mnQJ06uB",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 학습할 피처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10f0dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T02:03:34.299443Z",
     "iopub.status.busy": "2025-09-19T02:03:34.299098Z",
     "iopub.status.idle": "2025-09-19T02:03:38.108339Z",
     "shell.execute_reply": "2025-09-19T02:03:38.106556Z",
     "shell.execute_reply.started": "2025-09-19T02:03:34.299420Z"
    },
    "id": "BuQ5lNcax1Y8",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_leakage_related = [\n",
    "    'last_fico_range_high', 'last_fico_range_low', 'total_pymnt_inv', 'last_pymnt_amnt', 'out_prncp_inv', 'out_prncp',\n",
    "    'total_rec_prncp', 'total_pymnt', 'recoveries', 'collection_recovery_fee', ]\n",
    "\n",
    "features_highly_correlated = ['fico_range_low', 'loan_amnt', 'funded_amnt_inv', 'funded_amnt', 'num_sats',\n",
    "                          'num_rev_tl_bal_gt_0', 'tot_cur_bal', 'total_bal_il', 'mo_sin_old_rev_tl_op']\n",
    "\n",
    "features_to_check = [\n",
    "    'delinq_2yrs', 'fico_range_high', 'inq_last_6mths', 'pub_rec',\n",
    "    'total_rec_late_fee', 'collections_12_mths_ex_med', 'acc_now_delinq', 'chargeoff_within_12_mths', 'delinq_amnt'\n",
    "]\n",
    "\n",
    "features_to_check_2nd = [\n",
    "    'open_acc_6m', 'open_act_il', 'open_il_12m', 'open_il_24m',\n",
    "    'open_rv_12m', 'open_rv_24m',\n",
    "    'inq_fi', 'total_cu_tl', 'inq_last_12m',\n",
    "]\n",
    "\n",
    "X = df.drop(['is_default'] + features_leakage_related + features_highly_correlated\n",
    "                           + features_to_check + features_to_check_2nd, axis=1)\n",
    "y = df['is_default']\n",
    "y = y.map({'Default':True, 'NonDefault':False})\n",
    "\n",
    "# Calculate scale_pos_weight for XGBoost and LightGBM (needed for Case 2 models)\n",
    "# scale_pos_weight = count(negative class) / count(positive class)\n",
    "classes = y.unique()\n",
    "neg_class_label = False # Or the actual label for the negative class if not boolean\n",
    "pos_class_label = True # Or the actual label for the positive class if not boolean\n",
    "\n",
    "if neg_class_label in y.value_counts() and pos_class_label in y.value_counts():\n",
    "    neg_count = y.value_counts()[neg_class_label]\n",
    "    pos_count = y.value_counts()[pos_class_label]\n",
    "    scale_pos_weight_value = neg_count / pos_count\n",
    "    print(f\"Calculated scale_pos_weight: {scale_pos_weight_value:.4f}\")\n",
    "else:\n",
    "     print(\"Warning: Could not find expected negative or positive class labels in y.\")\n",
    "     scale_pos_weight_value = 1.0 # Default to 1 if labels not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0b463",
   "metadata": {
    "id": "i-4QckPK06uE",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Define the models for Case 1 (SMOTE used in pipeline, no class weight in model)\n",
    "models_case1 = [{\"name\": \"Logistic Regression\",\n",
    "                 \"model\": LogisticRegression(random_state=42, solver='liblinear', n_jobs=-1)},\n",
    "                {\"name\": \"Random Forest\",'\n",
    "                 \"model\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)},\n",
    "                {\"name\": \"XGBoost\",\n",
    "                 \"model\": xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss',\n",
    "                                            use_label_encoder=False, random_state=42, n_jobs=-1)},\n",
    "                {\"name\": \"LightGBM\",\n",
    "                 \"model\": lgb.LGBMClassifier(objective='binary', metric='binary_logloss', random_state=42, n_jobs=-1)}\n",
    "]\n",
    "\n",
    "# Define the models for Case 2 (No SMOTE, class weight in model)\n",
    "models_case2 = [{\"name\": \"Logistic Regression\",\n",
    "                 \"model\": LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced', n_jobs=-1)},\n",
    "                {\"name\": \"Random Forest\",\n",
    "                 \"model\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)},\n",
    "                {\"name\": \"XGBoost\",\n",
    "                 \"model\": xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False,\n",
    "                                            random_state=42, n_jobs=-1, scale_pos_weight=scale_pos_weight_value)},\n",
    "                {\"name\": \"LightGBM\",\n",
    "                 \"model\": lgb.LGBMClassifier(objective='binary', metric='binary_logloss', random_state=42, n_jobs=-1,\n",
    "                                             scale_pos_weight=scale_pos_weight_value)}\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize dictionary to store results, nested by case\n",
    "result = {\"Case 1 (SMOTE)\": {}, \"Case 2 (Class Weight)\": {}}\n",
    "\n",
    "# Train and evaluate models for Case 1 (using train_evaluate_and_get_metrics with SMOTE)\n",
    "print(\"\\n--- Training and Evaluating: Case 1 (SMOTE) ---\")\n",
    "\n",
    "for model_info in models_case1:\n",
    "    print(f\"\\n--- Training and Evaluating: {model_info['name']} (Case 1) ---\")\n",
    "    trained_pipeline, metrics = train_evaluate_and_get_metrics(model_info['model'], X, y)\n",
    "\n",
    "    if metrics:\n",
    "        result[\"Case 1 (SMOTE)\"][model_info['name']] = metrics\n",
    "        print(f\"--- Finished: {model_info['name']} (Case 1) ---\")\n",
    "        print(f\"Metrics stored: {metrics}\")\n",
    "    else:\n",
    "        print(f\"--- Finished: {model_info['name']} (Case 1) (Metrics not available) ---\")\n",
    "\n",
    "\n",
    "# Train and evaluate models for Case 2 (using train_evaluate_and_get_metrics_without_smote)\n",
    "print(\"\\n--- Training and Evaluating: Case 2 (Class Weight) ---\")\n",
    "\n",
    "for model_info in models_case2:\n",
    "    print(f\"\\n--- Training and Evaluating: {model_info['name']} (Case 2) ---\")\n",
    "    trained_pipeline, metrics = train_evaluate_and_get_metrics_without_smote(model_info['model'], X, y)\n",
    "\n",
    "    if metrics:\n",
    "        result[\"Case 2 (Class Weight)\"][model_info['name']] = metrics\n",
    "        print(f\"--- Finished: {model_info['name']} (Case 2) ---\")\n",
    "        print(f\"Metrics stored: {metrics}\")\n",
    "    else:\n",
    "        print(f\"--- Finished: {model_info['name']} (Case 2) (Metrics not available) ---\")\n",
    "\n",
    "\n",
    "# Display the results dictionary\n",
    "print(\"\\n--- Model Performance Metrics Summary ---\")\n",
    "display(result)\n",
    "\n",
    "\n",
    "# -- 결과저장 --\n",
    "#--- Model Performance Metrics Summary ---\n",
    "# {'Case 1 (SMOTE)': {'Logistic Regression': {'Accuracy': 0.7348342622469932,\n",
    "#    'Precision': 0.23753360095681314,\n",
    "#    'Recall': 0.7167256226104978,\n",
    "#    'F1-Score': 0.3568137751925574},\n",
    "#   'Random Forest': {'Accuracy': 0.8977559401584042,\n",
    "#    'Precision': 0.5261023821591485,\n",
    "#    'Recall': 0.03708864830099689,\n",
    "#    'F1-Score': 0.06929238985313751},\n",
    "#   'LightGBM': {'Accuracy': 0.8999156644177178,\n",
    "#    'Precision': 0.5579176431201874,\n",
    "#    'Recall': 0.11909100653875013,\n",
    "#    'F1-Score': 0.1962839727923206}},\n",
    "#  'Case 2 (Class Weight)': {'Logistic Regression': {'Accuracy': 0.7323041947785274,\n",
    "#    'Precision': 0.2368017960080914,\n",
    "#    'Recall': 0.7236216814949799,\n",
    "#    'F1-Score': 0.3568319971808651},\n",
    "#   'Random Forest': {'Accuracy': 0.8978952772073922,\n",
    "#    'Precision': 0.6153846153846154,\n",
    "#    'Recall': 0.013434809018472863,\n",
    "#    'F1-Score': 0.026295545143016994},\n",
    "#   'LightGBM': {'Accuracy': 0.7851312701672044,\n",
    "#    'Precision': 0.2935861473729636,\n",
    "#    'Recall': 0.7778611498195591,\n",
    "#    'F1-Score': 0.4262818316216137}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f645fcc8",
   "metadata": {
    "id": "KJWpWPud06uG",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2nd\n",
    "데이터 탐색 결과로 삭제한 특징을 학습에 포함시킨 후 재학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49c919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-19T02:04:20.350122Z",
     "iopub.status.busy": "2025-09-19T02:04:20.349783Z"
    },
    "id": "Wj6QDt-006uG",
    "outputId": "b80adfc8-7b1a-4fbb-81d6-bf183fda0be7",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Calculate scale_pos_weight for XGBoost and LightGBM (needed for Case 2 models)\n",
    "# scale_pos_weight = count(negative class) / count(positive class)\n",
    "classes = y.unique()\n",
    "neg_class_label = False # Or the actual label for the negative class if not boolean\n",
    "pos_class_label = True # Or the actual label for the positive class if not boolean\n",
    "\n",
    "if neg_class_label in y.value_counts() and pos_class_label in y.value_counts():\n",
    "    neg_count = y.value_counts()[neg_class_label]\n",
    "    pos_count = y.value_counts()[pos_class_label]\n",
    "    scale_pos_weight_value = neg_count / pos_count\n",
    "    print(f\"Calculated scale_pos_weight: {scale_pos_weight_value:.4f}\")\n",
    "else:\n",
    "     print(\"Warning: Could not find expected negative or positive class labels in y.\")\n",
    "     scale_pos_weight_value = 1.0 # Default to 1 if labels not found\n",
    "\n",
    "# Define the models for Case 1 (SMOTE used in pipeline, no class weight in model)\n",
    "models_case1 = [{\"name\": \"Logistic Regression\",\n",
    "                 \"model\": LogisticRegression(random_state=42, solver='liblinear', n_jobs=-1)},\n",
    "                {\"name\": \"Random Forest\",'\n",
    "                 \"model\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)},\n",
    "                {\"name\": \"XGBoost\",\n",
    "                 \"model\": xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss',\n",
    "                                            use_label_encoder=False, random_state=42, n_jobs=-1)},\n",
    "                {\"name\": \"LightGBM\",\n",
    "                 \"model\": lgb.LGBMClassifier(objective='binary', metric='binary_logloss', random_state=42, n_jobs=-1)}\n",
    "]\n",
    "\n",
    "# Define the models for Case 2 (No SMOTE, class weight in model)\n",
    "models_case2 = [{\"name\": \"Logistic Regression\",\n",
    "                 \"model\": LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced', n_jobs=-1)},\n",
    "                {\"name\": \"Random Forest\",\n",
    "                 \"model\": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)},\n",
    "                {\"name\": \"XGBoost\",\n",
    "                 \"model\": xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False,\n",
    "                                            random_state=42, n_jobs=-1, scale_pos_weight=scale_pos_weight_value)},\n",
    "                {\"name\": \"LightGBM\",\n",
    "                 \"model\": lgb.LGBMClassifier(objective='binary', metric='binary_logloss', random_state=42, n_jobs=-1,\n",
    "                                             scale_pos_weight=scale_pos_weight_value)}\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize dictionary to store results, nested by case\n",
    "result = {\"Case 1 (SMOTE)\": {}, \"Case 2 (Class Weight)\": {}}\n",
    "\n",
    "# Train and evaluate models for Case 1 (using train_evaluate_and_get_metrics with SMOTE)\n",
    "print(\"\\n--- Training and Evaluating: Case 1 (SMOTE) ---\")\n",
    "\n",
    "for model_info in models_case1:\n",
    "    print(f\"\\n--- Training and Evaluating: {model_info['name']} (Case 1) ---\")\n",
    "    trained_pipeline, metrics = train_evaluate_and_get_metrics(model_info['model'], X, y)\n",
    "\n",
    "    if metrics:\n",
    "        result[\"Case 1 (SMOTE)\"][model_info['name']] = metrics\n",
    "        print(f\"--- Finished: {model_info['name']} (Case 1) ---\")\n",
    "        print(f\"Metrics stored: {metrics}\")\n",
    "    else:\n",
    "        print(f\"--- Finished: {model_info['name']} (Case 1) (Metrics not available) ---\")\n",
    "\n",
    "\n",
    "# Train and evaluate models for Case 2 (using train_evaluate_and_get_metrics_without_smote)\n",
    "print(\"\\n--- Training and Evaluating: Case 2 (Class Weight) ---\")\n",
    "\n",
    "for model_info in models_case2:\n",
    "    print(f\"\\n--- Training and Evaluating: {model_info['name']} (Case 2) ---\")\n",
    "    trained_pipeline, metrics = train_evaluate_and_get_metrics_without_smote(model_info['model'], X, y)\n",
    "\n",
    "    if metrics:\n",
    "        result[\"Case 2 (Class Weight)\"][model_info['name']] = metrics\n",
    "        print(f\"--- Finished: {model_info['name']} (Case 2) ---\")\n",
    "        print(f\"Metrics stored: {metrics}\")\n",
    "    else:\n",
    "        print(f\"--- Finished: {model_info['name']} (Case 2) (Metrics not available) ---\")\n",
    "\n",
    "\n",
    "# Display the results dictionary\n",
    "print(\"\\n--- Model Performance Metrics Summary ---\")\n",
    "display(result)\n",
    "\n",
    "\n",
    "# -- 결과저장 --\n",
    "#--- Model Performance Metrics Summary ---\n",
    "# {'Case 1 (SMOTE)': {'Logistic Regression': {'Accuracy': 0.7348342622469932,\n",
    "#    'Precision': 0.23753360095681314,\n",
    "#    'Recall': 0.7167256226104978,\n",
    "#    'F1-Score': 0.3568137751925574},\n",
    "#   'Random Forest': {'Accuracy': 0.8977559401584042,\n",
    "#    'Precision': 0.5261023821591485,\n",
    "#    'Recall': 0.03708864830099689,\n",
    "#    'F1-Score': 0.06929238985313751},\n",
    "#   'LightGBM': {'Accuracy': 0.8999156644177178,\n",
    "#    'Precision': 0.5579176431201874,\n",
    "#    'Recall': 0.11909100653875013,\n",
    "#    'F1-Score': 0.1962839727923206}},\n",
    "#  'Case 2 (Class Weight)': {'Logistic Regression': {'Accuracy': 0.7323041947785274,\n",
    "#    'Precision': 0.2368017960080914,\n",
    "#    'Recall': 0.7236216814949799,\n",
    "#    'F1-Score': 0.3568319971808651},\n",
    "#   'Random Forest': {'Accuracy': 0.8978952772073922,\n",
    "#    'Precision': 0.6153846153846154,\n",
    "#    'Recall': 0.013434809018472863,\n",
    "#    'F1-Score': 0.026295545143016994},\n",
    "#   'LightGBM': {'Accuracy': 0.7851312701672044,\n",
    "#    'Precision': 0.2935861473729636,\n",
    "#    'Recall': 0.7778611498195591,\n",
    "#    'F1-Score': 0.4262818316216137}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91a3b69",
   "metadata": {
    "id": "tM3pxqUh06uH",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8299094,
     "sourceId": 13101426,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.925324,
   "end_time": "2025-09-19T05:21:24.056121",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-19T05:21:14.130797",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
